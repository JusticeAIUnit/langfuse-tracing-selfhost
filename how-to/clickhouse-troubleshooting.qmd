---
title: "ClickHouse and ZooKeeper Troubleshooting"
format:
  html:
    toc: true
diataxis:
  type: "how-to"
  purpose: "Problem-oriented - Diagnose and resolve ClickHouse replication and ZooKeeper coordination issues"
categories: ["Infrastructure & Deployment", "Troubleshooting", "Database"]
---

::: {.callout-note icon="false"}
**üõ†Ô∏è How-to Guide** - This guide helps you diagnose and resolve issues specific to ClickHouse replication and ZooKeeper coordination in LangFuse deployments.
:::

**Problem**: You're experiencing data inconsistencies, replication failures, or image pull issues related to ClickHouse and ZooKeeper components.

**Solution**: This comprehensive guide covers the most critical ClickHouse and ZooKeeper issues based on real production troubleshooting experience.

::: {.callout-important}
**Critical Understanding**: ClickHouse replication issues can cause inconsistent UI behavior, where refreshing the page shows different data each time. This is typically caused by UUID mismatches between replicas.
:::

## Prerequisites

Before troubleshooting ClickHouse issues, ensure you understand the replication architecture. See the [ClickHouse Replication Architecture](../explanation/clickhouse-replication-architecture.qmd) guide for comprehensive details on: 

- Complete table inventory and UI dependencies
- ZooKeeper coordination mechanics  
- Replication process and failure scenarios
- Production architecture considerations

## ClickHouse Migration System Issues

### Migration System Failures

**Error Pattern**: `error: Dirty database version X. Fix and force version.`

**Symptoms**: 

- `langfuse-web` pods in `CrashLoopBackOff` state
- Service completely unavailable
- Migration system reports "dirty" database state

**Root Cause**: ClickHouse cluster migration state inconsistency across shards, often combined with incorrect Helm configuration preventing environment variables from being applied.

#### Diagnostic Steps

**Step 1: Check Pod Status and Logs**
```bash
# Check pod status
kubectl get pods -n langfuse

# Check web pod logs for migration errors
kubectl logs -n langfuse deployment/langfuse-web --tail=50 | grep -i "migration\|dirty\|version"
```

**Step 2: Inspect ClickHouse Migration State**
```bash
# Connect to ClickHouse and check migration state across all shards
kubectl exec -n langfuse langfuse-clickhouse-shard0-0 -- clickhouse-client -q "
SELECT hostName(), version, dirty, sequence 
FROM schema_migrations 
ORDER BY sequence;"
```

**Expected Problem Pattern**:
```
Shard 0-0: version=1, dirty=1 (dirty state)
Shard 0-1: version=1, dirty=1 (dirty state)  
Shard 0-2: version=1, dirty=1 (dirty state)
```

**Step 3: Verify Environment Variables**
```bash
# Check if migration disable flag is actually applied
kubectl exec -n langfuse deployment/langfuse-web -- env | grep LANGFUSE_AUTO_CLICKHOUSE_MIGRATION_DISABLED

# If empty or "false", environment variables aren't being applied correctly
```

#### Resolution: Cluster State Synchronization

**Fix Migration State Across All Shards**:
```bash
# Connect to each ClickHouse shard and synchronize state
for shard in 0 1 2; do
  echo "Fixing shard $shard..."
  kubectl exec -n langfuse langfuse-clickhouse-shard0-$shard -- clickhouse-client -q "
    -- Clear dirty flags
    ALTER TABLE schema_migrations UPDATE dirty = 0 WHERE version = 1;
    
    -- Remove invalid version numbers (if any exist)
    DELETE FROM schema_migrations WHERE version > 100;
    
    -- Ensure clean state
    INSERT INTO schema_migrations (version, dirty, sequence) VALUES (1, 0, 1) 
    ON DUPLICATE KEY UPDATE dirty = 0;
  "
done
```

**Verify Synchronization**:
```bash
# All shards should show identical, clean state
kubectl exec -n langfuse langfuse-clickhouse-shard0-0 -- clickhouse-client -q "
SELECT hostName(), version, dirty, sequence 
FROM schema_migrations 
ORDER BY sequence;"
```

#### Prevention and Monitoring

**Regular Cluster State Verification**:
```bash
# Add to monitoring scripts
kubectl exec -n langfuse langfuse-clickhouse-shard0-0 -- clickhouse-client -q "
SELECT 
  hostName(),
  version,
  dirty,
  CASE WHEN dirty = 1 THEN '‚ö†Ô∏è DIRTY' ELSE '‚úÖ CLEAN' END as status
FROM schema_migrations 
ORDER BY sequence;"
```

**Migration Disable Configuration**:

If you need to disable auto-migrations (recommended for production), ensure the environment variable is properly configured:

```bash
# Verify migration disable is working
kubectl exec -n langfuse deployment/langfuse-web -- env | grep LANGFUSE_AUTO_CLICKHOUSE_MIGRATION_DISABLED

# Should show: LANGFUSE_AUTO_CLICKHOUSE_MIGRATION_DISABLED=true
```

For Helm configuration issues with environment variables, see the [Email Notifications Guide](email-notifications.qmd#helm-configuration-issues-with-smtp) which covers the `additionalEnv` vs `extraEnv` configuration path issue.

## ClickHouse Replication Synchronization Issues

### Problem Symptoms

**UI Behavior**:

- Inconsistent data displayed on UI refresh (different trace counts each time)
- "Empty traces" appearing intermittently
- Internal server errors: `traces.filterOptions`, `traces.byIdWithObservationsAndScores`
- Sessions and Observations tables showing malformed column names

**Visual Indicators**:

When ClickHouse replication issues occur, the LangFuse UI may show degraded functionality:

![ZooKeeper OOM in LangFuse UI](../images/troubleshooting/langfuse-zookeeper-oom-ui.png)

**Key UI Indicators**:

- **Trace Loading Issues**: Traces may show "Not Found" errors or fail to load completely
- **Null Input/Output**: Trace data appears as `null` values instead of actual content
- **Empty Metadata**: Trace metadata sections show empty JSON objects `{}`
- **Timeline Disruption**: Trace timeline may be incomplete or missing spans

### Symptom-Based Troubleshooting

Use this table to quickly identify the root cause based on specific UI symptoms:

| Symptom | Root Cause | Immediate Action |
|---------|------------|------------------|
| "Internal Server Error - sessions.all" | Missing `scores` table on replicas | Recreate `scores` table with correct UUID |
| "Trace not found" errors (intermittent) | Data inconsistency across replicas | Verify data counts, force sync if needed |
| Dashboard shows no metrics | Missing analytics views | Recreate `analytics_*` views |
| Environment filter dropdown empty | Missing `project_environments` table | Recreate `project_environments` table |
| File attachments fail to load/upload | Missing `blob_storage_file_log` table | Recreate `blob_storage_file_log` table |
| Inconsistent data on page refresh | UUID mismatch between replicas | Full split-brain resolution required |
| Sessions page crashes completely | Missing materialized views | Recreate `project_environments_*_mv` views |
| Event tracking/logs missing | Missing `event_log` table | Recreate `event_log` table |
| Trace details show empty timeline | Missing `observations` table | Recreate `observations` table |
| Zero trace counts across all pages | Missing `traces` table | Recreate `traces` table |

::: {.callout-tip}
**Quick Diagnosis**: If you see inconsistent data on page refresh, this is almost always a split-brain issue requiring full resolution. Other symptoms typically indicate missing specific tables. For complete table-to-UI mapping, see the [ClickHouse Replication Architecture](../explanation/clickhouse-replication-architecture.qmd) guide.
:::

### Root Cause Analysis

**Primary Cause**: After recreating ClickHouse replicas with fresh PersistentVolumeClaims (PVCs), new replicas initialize tables with **different UUIDs** than the original replica. This creates **multiple isolated replication groups** instead of one unified cluster.

**Example of Broken State**:
```
‚úó BROKEN STATE:
- shard0-0: UUID bd05a9b1-... (34,264 traces)
- shard0-1: UUID bcdf1671-... (34,109 traces) 
- shard0-2: UUID bcdf1671-... (34,184 traces)
```

Each replica contains different data, and the load balancer randomly directs queries to different replicas, causing inconsistent UI behavior.

### Diagnostic Commands

**Check Replication Status**:
```bash
# Verify current replication health
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client \
  --password=$PASSWORD -q \
  "SELECT table, total_replicas, active_replicas FROM system.replicas WHERE database='default'"

# Expected output: total_replicas=3, active_replicas=3 for all tables
```

**Verify UUID Consistency**:
```bash
# Check UUIDs across all replicas
for pod in langfuse-clickhouse-shard0-{0,1,2}; do
  echo "=== $pod ==="
  kubectl exec $pod -n langfuse -- clickhouse-client \
    --password=$PASSWORD -q \
    "SELECT table, zookeeper_path FROM system.replicas WHERE database='default'"
done

# All replicas should show identical UUIDs in zookeeper_path
```

**Check Data Consistency**:
```bash
# Verify row counts across replicas
for pod in langfuse-clickhouse-shard0-{0,1,2}; do
  echo "$pod:"
  kubectl exec $pod -n langfuse -- clickhouse-client \
    --password=$PASSWORD -q "SELECT count() FROM traces"
done

# All replicas should show the same count
```

### Resolution Steps

::: {.callout-warning}
**Data Safety**: This procedure involves scaling down to a single replica. Ensure you identify the replica with the most complete data before proceeding.
:::

#### Step 1: Scale Down to Single Replica

```bash
# Scale down to keep only the primary replica
kubectl scale statefulset langfuse-clickhouse-shard0 --replicas=1 -n langfuse

# Verify only shard0-0 is running
kubectl get pods -n langfuse | grep clickhouse
```

Keep only `shard0-0` (the replica with the most complete data) to establish a single source of truth.

#### Step 2: Delete Corrupted PVCs

```bash
# Remove PVCs for the corrupted replicas
kubectl delete pvc data-langfuse-clickhouse-shard0-1 data-langfuse-clickhouse-shard0-2 -n langfuse

# Verify PVCs are deleted
kubectl get pvc -n langfuse | grep clickhouse
```

#### Step 3: Comprehensive ZooKeeper Metadata Cleanup

::: {.callout-warning}
**Critical Step**: Stale ZooKeeper metadata prevents proper replication. You must clean metadata for **every table UUID** and **every replica** combination.
:::

**Step 3a: Get All Table UUIDs**

```bash
# Get ALL table UUIDs from healthy replica (critical - don't miss any)
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --password=$PASSWORD -q \
  "SELECT table, substring(zookeeper_path, 20, 36) as uuid FROM system.replicas WHERE database='default'"

# Example output:
# traces                bd05a9b1-...
# observations          c8f2e1a3-...
# scores                379293dc-...
# blob_storage_file_log 9f10e361-...
# project_environments  50f051d7-...
# schema_migrations     a1b2c3d4-...
```

**Step 3b: Automated Cleanup Script**

```bash
# Create cleanup script for systematic metadata removal
cat > cleanup_zookeeper_metadata.sh << 'EOF'
#!/bin/bash
set -e

PASSWORD="$1"
if [ -z "$PASSWORD" ]; then
  echo "Usage: $0 <clickhouse-password>"
  exit 1
fi

echo "=== Getting all table UUIDs ==="
UUIDS=$(kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
  "SELECT substring(zookeeper_path, 20, 36) FROM system.replicas WHERE database='default'" | tr '\n' ' ')

echo "Found UUIDs: $UUIDS"

echo "=== Cleaning ZooKeeper metadata ==="
for uuid in $UUIDS; do
  echo "Cleaning UUID: $uuid"
  
  # Clean shard0-1 metadata
  kubectl exec langfuse-zookeeper-0 -n langfuse -- zkCli.sh -server localhost:2181 \
    deleteall /clickhouse/tables/$uuid/shard0/replicas/langfuse-clickhouse-shard0-1 2>/dev/null || true
  
  # Clean shard0-2 metadata  
  kubectl exec langfuse-zookeeper-0 -n langfuse -- zkCli.sh -server localhost:2181 \
    deleteall /clickhouse/tables/$uuid/shard0/replicas/langfuse-clickhouse-shard0-2 2>/dev/null || true
    
  echo "  ‚úì Cleaned metadata for UUID: $uuid"
done

echo "=== Verifying cleanup ==="
for uuid in $UUIDS; do
  replicas=$(kubectl exec langfuse-zookeeper-0 -n langfuse -- zkCli.sh -server localhost:2181 \
    ls /clickhouse/tables/$uuid/shard0/replicas 2>/dev/null | grep -o '\[.*\]' || echo "[]")
  echo "UUID $uuid replicas: $replicas"
  # Should only show: [langfuse-clickhouse-shard0-0]
done

echo "=== ZooKeeper cleanup complete ==="
EOF

chmod +x cleanup_zookeeper_metadata.sh

# Run the cleanup script
./cleanup_zookeeper_metadata.sh "$PASSWORD"
```

**Step 3c: Manual Verification**

```bash
# Verify cleanup for each UUID (replace with actual UUIDs from step 3a)
kubectl exec langfuse-zookeeper-0 -n langfuse -- zkCli.sh -server localhost:2181 \
  ls /clickhouse/tables/bd05a9b1-9dc0-417f-93b8-22fbda0e61ba/shard0/replicas

# Should only show: [langfuse-clickhouse-shard0-0]
# If you see shard0-1 or shard0-2, the cleanup failed and must be repeated
```

#### Step 4: Scale Back Up

```bash
# Scale back to 3 replicas
kubectl scale statefulset langfuse-clickhouse-shard0 --replicas=3 -n langfuse

# Wait for pods to start
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=clickhouse -n langfuse --timeout=300s
```

#### Step 5: Recreate Tables with Correct UUIDs

For each of the 6 replicated tables, recreate on the new replicas using the **explicit UUID from shard0-0**:

**Get the correct UUID**:
```bash
# Get the UUID for the traces table (example)
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client \
  --password=$PASSWORD -q \
  "SELECT zookeeper_path FROM system.replicas WHERE table='traces' AND database='default'"
```

**Example for `traces` table**:
```sql
-- Execute on shard0-1 and shard0-2
CREATE TABLE IF NOT EXISTS traces (
    -- [Use the complete schema from the original table]
) ENGINE = ReplicatedReplacingMergeTree(
    '/clickhouse/tables/bd05a9b1-9dc0-417f-93b8-22fbda0e61ba/shard0',  -- ‚Üê Use explicit UUID from shard0-0
    '{replica}', 
    event_ts, 
    is_deleted
)
PARTITION BY toYYYYMM(timestamp)
PRIMARY KEY (project_id, toDate(timestamp))
ORDER BY (project_id, toDate(timestamp), id)
SETTINGS index_granularity = 8192
```

**Repeat for all replicated tables**:
- `observations`
- `scores` 
- `blob_storage_file_log`
- `project_environments`
- `schema_migrations`

#### Step 6: Create Supporting Objects

On both new replicas, create the supporting views and tables:

**Analytics Views**:
```sql
CREATE VIEW analytics_observations AS 
SELECT 
    toStartOfHour(timestamp) as hour,
    project_id,
    count() as observation_count
FROM observations 
GROUP BY hour, project_id;

CREATE VIEW analytics_scores AS 
SELECT 
    toStartOfHour(timestamp) as hour,
    project_id,
    count() as score_count
FROM scores 
GROUP BY hour, project_id;

CREATE VIEW analytics_traces AS 
SELECT 
    toStartOfHour(timestamp) as hour,
    project_id,
    count() as trace_count
FROM traces 
GROUP BY hour, project_id;
```

**Event Log Table**:
```sql
CREATE TABLE event_log (
    timestamp DateTime,
    event_type String,
    project_id String,
    data String
) ENGINE = MergeTree 
ORDER BY (timestamp, project_id);
```

**Materialized Views**:
```sql
CREATE MATERIALIZED VIEW project_environments_observations_mv TO project_environments AS 
SELECT 
    project_id,
    'observations' as metric_type,
    count() as value
FROM observations 
GROUP BY project_id;

CREATE MATERIALIZED VIEW project_environments_scores_mv TO project_environments AS 
SELECT 
    project_id,
    'scores' as metric_type,
    count() as value
FROM scores 
GROUP BY project_id;

CREATE MATERIALIZED VIEW project_environments_traces_mv TO project_environments AS 
SELECT 
    project_id,
    'traces' as metric_type,
    count() as value
FROM traces 
GROUP BY project_id;
```

#### Step 7: Force Historical Data Synchronization

::: {.callout-important}
**Critical Issue**: New replicas only receive data from the point they join the cluster. Historical data remains only on the original replica and must be force-synchronized.
:::

**Problem**: After recreating replicas with fresh PVCs, they start with empty tables and only get **new** data going forward. This causes data inconsistency where different replicas have different row counts.

**Solution**: Use `DETACH TABLE` / `ATTACH TABLE` operations to trigger proper replication log replay.

**Step 7a: Force Synchronization for All Replicated Tables**

```bash
# Create synchronization script
cat > force_historical_sync.sh << 'EOF'
#!/bin/bash
set -e

PASSWORD="$1"
if [ -z "$PASSWORD" ]; then
  echo "Usage: $0 <clickhouse-password>"
  exit 1
fi

REPLICATED_TABLES=("traces" "observations" "scores" "blob_storage_file_log" "project_environments" "schema_migrations")

echo "=== Forcing Historical Data Synchronization ==="

for replica in langfuse-clickhouse-shard0-1 langfuse-clickhouse-shard0-2; do
  echo "--- Synchronizing $replica ---"
  
  for table in "${REPLICATED_TABLES[@]}"; do
    echo "  Syncing table: $table"
    
    # Detach table to reset replication state
    kubectl exec $replica -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
      "DETACH TABLE $table" || echo "    Warning: DETACH failed for $table"
    
    # Wait a moment for detach to complete
    sleep 2
    
    # Attach table to trigger replication log replay
    kubectl exec $replica -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
      "ATTACH TABLE $table" || echo "    Warning: ATTACH failed for $table"
    
    # Force immediate synchronization
    kubectl exec $replica -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
      "SYSTEM SYNC REPLICA $table" || echo "    Warning: SYNC failed for $table"
    
    echo "    ‚úì Synchronized $table on $replica"
  done
done

echo "=== Historical synchronization complete ==="
EOF

chmod +x force_historical_sync.sh

# Run the synchronization script
./force_historical_sync.sh "$PASSWORD"
```

**Step 7b: Verify Data Consistency**

```bash
# Critical: Verify identical data across all replicas
echo "=== Data Consistency Verification ==="
for table in traces observations scores blob_storage_file_log project_environments schema_migrations; do
  echo "--- $table ---"
  for pod in langfuse-clickhouse-shard0-{0,1,2}; do
    count=$(kubectl exec $pod -n langfuse -- clickhouse-client --password=$PASSWORD -q \
      "SELECT count() FROM $table" 2>/dev/null || echo "ERROR")
    echo "  $pod: $count rows"
  done
  echo
done

# All counts MUST be identical. Different counts indicate incomplete replication.
```

**Step 7c: Alternative Method - Force Replication Log Replay**

If DETACH/ATTACH doesn't work, try forcing replication log replay:

```bash
# Check current replication log positions
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --password=$PASSWORD -q \
  "SELECT replica_name, log_max_index, log_pointer FROM system.replicas WHERE table='traces'"

# Force log replay on new replicas for each table
for replica in langfuse-clickhouse-shard0-1 langfuse-clickhouse-shard0-2; do
  for table in traces observations scores blob_storage_file_log project_environments schema_migrations; do
    kubectl exec $replica -n langfuse -- clickhouse-client --password=$PASSWORD -q \
      "SYSTEM SYNC REPLICA $table"
  done
done
```

#### Step 8: Verify Complete Replication

```bash
# Check replication health
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client \
  --password=$PASSWORD -q \
  "SELECT 
    table,
    replica_name,
    is_leader,
    total_replicas,
    active_replicas,
    queue_size,
    log_max_index,
    log_pointer
  FROM system.replicas 
  WHERE database='default'
  ORDER BY table"

# Verify data consistency across all replicas
for pod in langfuse-clickhouse-shard0-{0,1,2}; do
  echo "$pod:"
  kubectl exec $pod -n langfuse -- clickhouse-client \
    --password=$PASSWORD -q \
    "SELECT 
      'traces' as table, count() as row_count FROM traces
    UNION ALL
    SELECT 'observations', count() FROM observations
    UNION ALL
    SELECT 'scores', count() FROM scores"
done
```

## ImagePullBackOff Issues

### Bitnami Registry Migration (Critical)

**Date**: August 28, 2025 - Bitnami migrated all versioned container images

**Symptoms**:

- Error: `docker.io/bitnami/<image>:<tag>: not found`
- Even "latest" tags fail for versioned images
- Working pods use cached images
- New node deployments fail

**Background**: 

Bitnami migrated versioned container images:

- **Old location**: `docker.io/bitnami/*`
- **New location**: `docker.io/bitnamilegacy/*` (for versioned tags)
- **Public catalog**: `docker.io/bitnami/*` (latest tags only)

### Diagnostic Commands

**Check Current Image References**:
```bash
# Check what images are being pulled
kubectl get statefulset -n langfuse -o yaml | grep "image:"

# Test pull from bitnami (will fail for versioned tags)
kubectl debug node/<node-name> -it --image=alpine -- \
  chroot /host crictl pull docker.io/bitnami/clickhouse:25.2.1-debian-12-r0

# Test pull from bitnamilegacy (should work)
kubectl debug node/<node-name> -it --image=alpine -- \
  chroot /host crictl pull docker.io/bitnamilegacy/clickhouse:25.2.1-debian-12-r0
```

**Check Pod Events**:
```bash
# Get detailed error information
kubectl describe pod <failing-pod> -n langfuse | grep -A 10 "Events:"

# Check which node is affected
kubectl get pods -n langfuse -o wide | grep ImagePullBackOff
```

### Resolution Options

#### Option A: Quick Fix - Use Legacy Registry

```bash
# Update Helm deployment to use legacy registry
helm upgrade langfuse oci://registry-1.docker.io/bitnamicharts/langfuse \
  -n langfuse \
  --reuse-values \
  --set clickhouse.image.registry=docker.io \
  --set clickhouse.image.repository=bitnamilegacy/clickhouse \
  --set clickhouse.zookeeper.image.registry=docker.io \
  --set clickhouse.zookeeper.image.repository=bitnamilegacy/zookeeper

# Verify the update
kubectl get statefulset -n langfuse -o yaml | grep "image:" | grep bitnamilegacy
```

#### Option B: Node-Specific Issues

If only specific nodes are affected:

```bash
# Check node conditions
kubectl describe node <node-name>

# Check disk space on node
kubectl debug node/<node-name> -it --image=alpine -- df -h /host

# Clean up unused images if disk pressure
kubectl debug node/<node-name> -it --image=alpine -- \
  chroot /host crictl rmi --prune

# If node is consistently problematic, consider replacement
kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```

### Registry Connectivity Issues

**Symptoms**:
- Error: `connection refused`, `timeout`, `network error`
- Intermittent failures
- Affects external registries

**Diagnostic Commands**:
```bash
# Test DNS resolution
kubectl debug node/<node-name> -it --image=busybox -- nslookup docker.io

# Test network connectivity
kubectl debug node/<node-name> -it --image=busybox -- ping -c 3 registry-1.docker.io

# Check NAT Gateway status (Azure)
az network nat gateway show --name <nat-gw> --resource-group <rg>
```

**Common Causes and Solutions**:

1. **NAT Gateway Exhaustion**:
```bash
# Check SNAT port usage
az monitor metrics list \
  --resource <nat-gw-id> \
  --metric "SNAT Connection Count"

# Add more public IPs if exhausted
az network nat gateway update \
  --name <nat-gw> \
  --resource-group <rg> \
  --public-ip-addresses ip1 ip2 ip3
```

2. **Network Policies**:
```bash
# Check for restrictive network policies
kubectl get networkpolicies -A

# Temporarily allow all egress for testing
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
  namespace: langfuse
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - {}
EOF
```

## Production Considerations and Complexity Warnings

### What to Expect in Production

::: {.callout-warning}
**Complexity Warning**: ClickHouse split-brain resolution is a high-complexity operation requiring deep understanding of distributed systems and careful execution.
:::

**Resolution Characteristics**:

- **Risk Level**: High - data loss possible if executed incorrectly
- **Dependencies**: kubectl access, ZooKeeper expertise, production downtime tolerance

**Success Factors**:

1. **Complete table inventory** - Missing any of the 13 tables causes UI failures
2. **Systematic ZooKeeper cleanup** - Stale metadata prevents proper replication
3. **Historical data synchronization** - New replicas don't get old data automatically
4. **Data consistency verification** - Must verify identical data across all replicas

### Production-Tested Verification Procedures

**Complete Table Verification Script**:
```bash
#!/bin/bash
# check_complete_tables.sh - Production-tested table verification

PASSWORD="$1"
if [ -z "$PASSWORD" ]; then
  echo "Usage: $0 <clickhouse-password>"
  exit 1
fi

REQUIRED_TABLES=(
  "traces"
  "observations" 
  "scores"
  "blob_storage_file_log"
  "project_environments"
  "schema_migrations"
  "analytics_traces"
  "analytics_observations"
  "analytics_scores"
  "project_environments_traces_mv"
  "project_environments_observations_mv"
  "project_environments_scores_mv"
  "event_log"
)

echo "=== Complete Table Verification ==="
for pod in langfuse-clickhouse-shard0-{0,1,2}; do
  echo "--- $pod ---"
  missing_tables=()
  
  for table in "${REQUIRED_TABLES[@]}"; do
    exists=$(kubectl exec $pod -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
      "SELECT count() FROM system.tables WHERE name='$table' AND database='default'" 2>/dev/null || echo "0")
    
    if [ "$exists" = "1" ]; then
      if [[ "$table" == *"_mv" ]] || [[ "$table" == "analytics_"* ]] || [[ "$table" == "event_log" ]]; then
        # Non-replicated tables - just check existence
        echo "  ‚úì $table: EXISTS"
      else
        # Replicated tables - check row count
        count=$(kubectl exec $pod -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
          "SELECT count() FROM $table" 2>/dev/null || echo "ERROR")
        echo "  ‚úì $table: $count rows"
      fi
    else
      echo "  ‚úó $table: MISSING"
      missing_tables+=("$table")
    fi
  done
  
  if [ ${#missing_tables[@]} -gt 0 ]; then
    echo "  ‚ö†Ô∏è  Missing tables on $pod: ${missing_tables[*]}"
  fi
  echo
done
```

**Replication Health Verification**:
```bash
#!/bin/bash
# check_replication_health.sh - Production-tested replication verification

PASSWORD="$1"
if [ -z "$PASSWORD" ]; then
  echo "Usage: $0 <clickhouse-password>"
  exit 1
fi

echo "=== Comprehensive Replication Health Check ==="

# Check basic replication status
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
  "SELECT 
    table,
    replica_name,
    is_leader,
    total_replicas,
    active_replicas,
    queue_size,
    log_max_index,
    log_pointer,
    CASE 
      WHEN total_replicas = active_replicas AND queue_size = 0 THEN 'HEALTHY'
      WHEN total_replicas = active_replicas AND queue_size > 0 THEN 'SYNCING'
      ELSE 'DEGRADED'
    END as status
  FROM system.replicas 
  WHERE database='default'
  ORDER BY table, replica_name"

echo
echo "=== Data Consistency Verification ==="

# Check data consistency for all replicated tables
REPLICATED_TABLES=("traces" "observations" "scores" "blob_storage_file_log" "project_environments" "schema_migrations")

for table in "${REPLICATED_TABLES[@]}"; do
  echo "--- $table ---"
  counts=()
  
  for pod in langfuse-clickhouse-shard0-{0,1,2}; do
    count=$(kubectl exec $pod -n langfuse -- clickhouse-client --password="$PASSWORD" -q \
      "SELECT count() FROM $table" 2>/dev/null || echo "ERROR")
    counts+=("$count")
    echo "  $pod: $count rows"
  done
  
  # Check if all counts are identical
  if [ "${counts[0]}" = "${counts[1]}" ] && [ "${counts[1]}" = "${counts[2]}" ] && [ "${counts[0]}" != "ERROR" ]; then
    echo "  ‚úì Data consistent across all replicas"
  else
    echo "  ‚úó Data inconsistency detected - split-brain issue!"
  fi
  echo
done
```

## Prevention and Monitoring

### Replication Monitoring

**Set up Regular Health Checks**:
```bash
# Create monitoring script
cat > check_clickhouse_health.sh << 'EOF'
#!/bin/bash
echo "=== ClickHouse Replication Health ==="
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client \
  --password=$CLICKHOUSE_PASSWORD -q \
  "SELECT table, total_replicas, active_replicas, 
   CASE WHEN total_replicas = active_replicas THEN 'HEALTHY' ELSE 'DEGRADED' END as status
   FROM system.replicas WHERE database='default'"

echo "=== Data Consistency Check ==="
for pod in langfuse-clickhouse-shard0-{0,1,2}; do
  count=$(kubectl exec $pod -n langfuse -- clickhouse-client \
    --password=$CLICKHOUSE_PASSWORD -q "SELECT count() FROM traces" 2>/dev/null || echo "ERROR")
  echo "$pod: $count traces"
done
EOF

chmod +x check_clickhouse_health.sh
```

**Direct SQL Health Checks**:

For manual monitoring or integration into custom monitoring systems:

```sql
-- Check comprehensive replication status
SELECT 
    table,
    total_replicas,
    active_replicas,
    queue_size,
    CASE 
        WHEN total_replicas = active_replicas AND queue_size = 0 THEN 'HEALTHY'
        WHEN total_replicas = active_replicas AND queue_size > 0 THEN 'SYNCING'
        ELSE 'DEGRADED'
    END as status
FROM system.replicas 
WHERE database = 'default'
ORDER BY table;

-- Check data consistency across all core tables
SELECT 
    'traces' as table, 
    count() as row_count 
FROM traces
UNION ALL
SELECT 'observations', count() FROM observations
UNION ALL  
SELECT 'scores', count() FROM scores
UNION ALL
SELECT 'blob_storage_file_log', count() FROM blob_storage_file_log
UNION ALL
SELECT 'project_environments', count() FROM project_environments
UNION ALL
SELECT 'schema_migrations', count() FROM schema_migrations;
```

**Table Creation Reference**:

When recreating tables, use the correct engine parameters:

```sql
-- Example: ReplicatedReplacingMergeTree engine
ENGINE = ReplicatedReplacingMergeTree(
    '/clickhouse/tables/{uuid}/shard0',  -- ZooKeeper path (must be identical)
    '{replica}',                         -- Replica name (unique per replica)
    event_ts,                           -- Version column for deduplication
    is_deleted                          -- Deletion flag column
)
```

**Automated Monitoring**:
```bash
# Run health check every 5 minutes
watch -n 300 ./check_clickhouse_health.sh
```

### Prevention Best Practices

1. **Never delete PVCs without proper backup** - Data and replication metadata are both lost
2. **Always verify UUID consistency** after scaling operations
3. **Monitor replication status** - Set up alerts for `active_replicas < total_replicas`
4. **Document all table UUIDs** before any maintenance operations
5. **Use ZooKeeper cleanup** before recreating replicas

### Image Pull Prevention

1. **Pin specific image versions** in Helm values
2. **Set up image pull monitoring and alerts**
3. **Subscribe to registry provider announcements** (Bitnami, Docker Hub)
4. **Implement image mirroring** to private registry (ACR)
5. **Test image pulls on all nodes** periodically

## Quick Reference Commands

### Emergency Diagnostics

```bash
# Quick health check
kubectl get pods -n langfuse | grep -E "(clickhouse|zookeeper)"

# Check replication status
kubectl exec langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client \
  --password=$PASSWORD -q "SELECT table, active_replicas FROM system.replicas"

# Check for image pull errors
kubectl get events -n langfuse --field-selector reason=Failed | grep -i "image"

# Test manual image pull
kubectl debug node/<node-name> -it --image=alpine -- \
  chroot /host crictl pull <full-image-path>
```

### Emergency Recovery

```bash
# Force pod restart
kubectl delete pod <pod-name> -n langfuse

# Scale down/up StatefulSet
kubectl scale statefulset langfuse-clickhouse-shard0 --replicas=0 -n langfuse
sleep 30
kubectl scale statefulset langfuse-clickhouse-shard0 --replicas=3 -n langfuse

# Check Helm values
helm get values langfuse -n langfuse
```

## Related Documentation

For understanding and broader deployment issues, see:

- **[ClickHouse Replication Architecture](../explanation/clickhouse-replication-architecture.qmd)** - Deep dive into replication mechanics, table relationships, and UI dependencies
- **[Database Architecture](../explanation/database-architecture.qmd)** - Understanding the dual database system
- **[Deployment Troubleshooting](deployment-troubleshooting.qmd)** - General Azure deployment issues
- **[Data Lifecycle Management](data-lifecycle-management.qmd)** - TTL rules for automatic data retention and cleanup
- **[Azure Architecture](../explanation/azure-architecture.qmd)** - Overall infrastructure components

---

*ClickHouse and ZooKeeper issues can be complex, but systematic diagnosis and understanding of the replication architecture enables effective resolution.*
