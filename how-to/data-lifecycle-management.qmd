---
title: "Data Lifecycle Management with TTL"
format:
  html:
    toc: true
diataxis:
  type: "how-to"
  purpose: "Problem-oriented - Implement automatic data retention policies for trace data"
categories: ["Infrastructure & Deployment", "Database", "Data Management"]
---

::: {.callout-note icon="false"}
**üõ†Ô∏è How-to Guide** - This guide helps you implement Time To Live (TTL) rules for automatic data lifecycle management in your LangFuse deployment, ensuring compliance with data protection retention policies.
:::

# Data Lifecycle Management with TTL

**Problem**: Your LangFuse deployment is accumulating large volumes of trace data over time, leading to storage costs, performance degradation, or non-compliance with data protection retention policies.

**Solution**: Implement ClickHouse TTL (Time To Live) rules to automatically manage data retention on a per-project basis, ensuring optimal performance and storage efficiency.

::: {.callout-tip}
**Quick Win**: TTL rules provide automated compliance with data protection policies while optimizing storage costs and performance.
:::

## Understanding TTL in LangFuse

### What TTL Does

TTL (Time To Live) rules in ClickHouse automatically delete data based on time-based conditions:

- **Automatic Cleanup** - Removes data older than specified retention period
- **Per-Project Control** - Different retention policies for different projects
- **Background Operation** - Runs during ClickHouse merge operations without impacting performance
- **Storage Optimization** - Reduces storage costs and improves query performance

### When to Use TTL

**Ideal Scenarios**:

- **Development/Testing Projects** - Short retention (1-7 days) for rapid iteration
- **High-Volume Production** - Medium retention (30-90 days) for operational data
- **Compliance Requirements** - Specific retention periods mandated by data protection policies
- **Regulatory Adherence** - Automated enforcement of maximum data retention periods

**Benefits**:

- **Cost Reduction** - Lower storage and compute costs
- **Performance Improvement** - Faster queries on smaller datasets  
- **Regulatory Compliance** - Automated adherence to data protection retention policies
- **Risk Mitigation** - Reduces exposure from retaining data beyond required periods

## Implementing TTL Rules

### Step 1: Identify Target Project

First, identify the project ID for which you want to implement TTL:

```bash
# Connect to ClickHouse and list projects with trace counts
kubectl exec -it langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --query "
SELECT 
    project_id,
    count() as trace_count,
    min(timestamp) as oldest_trace,
    max(timestamp) as newest_trace
FROM traces 
GROUP BY project_id 
ORDER BY trace_count DESC;"
```

### Step 2: Choose Retention Period

Select an appropriate retention period based on your needs:

| Use Case | Recommended Retention | Rationale |
|----------|----------------------|-----------|
| **Development/Testing** | 1-3 days | Rapid iteration, frequent deployments |
| **Production Monitoring** | 7-30 days | Operational visibility, incident response |
| **Compliance/Audit** | 90-365 days | Regulatory requirements |
| **Data Protection Policy** | As mandated | GDPR, privacy regulations, organizational policy |

### Step 3: Apply TTL Rules

Apply TTL rules to all three main data tables. Replace `YOUR_PROJECT_ID` with your actual project ID and adjust the retention period as needed:

```sql
-- Connect to ClickHouse
kubectl exec -it langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client

-- Apply TTL to traces table (adjust INTERVAL as needed)
ALTER TABLE traces 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 30 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID';

-- Apply TTL to observations table
ALTER TABLE observations 
MODIFY TTL 
    toDateTime(start_time) + INTERVAL 30 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID';

-- Apply TTL to scores table
ALTER TABLE scores 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 30 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID';
```

::: {.callout-warning}
**Irreversible Operation**: TTL rules will permanently delete data. Ensure you have backups if you need to retain historical data for compliance or analysis purposes.
:::

### Step 4: Force Immediate Cleanup (Optional)

If you want to immediately remove old data rather than waiting for background merges:

```sql
-- Force immediate cleanup (this may take several minutes)
OPTIMIZE TABLE traces FINAL;
OPTIMIZE TABLE observations FINAL;
OPTIMIZE TABLE scores FINAL;
```

::: {.callout-note}
**Performance Impact**: `OPTIMIZE TABLE FINAL` can be resource-intensive. Run during maintenance windows for production systems.
:::

## Advanced TTL Configurations

### Multiple Retention Periods

You can set different retention periods for different projects:

```sql
-- Short retention for development project
ALTER TABLE traces 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 3 DAY 
    DELETE WHERE project_id = 'dev_project_id';

-- Longer retention for production project  
ALTER TABLE traces 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 90 DAY 
    DELETE WHERE project_id = 'prod_project_id';
```

### Conditional TTL Rules

Apply TTL only to specific data patterns:

```sql
-- Remove only traces with specific tags after 7 days
ALTER TABLE traces 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 7 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID' 
    AND tags LIKE '%test%';
```

### Graduated TTL Policies

Implement tiered retention with different rules:

```sql
-- Remove detailed observations after 7 days, keep traces for 30 days
ALTER TABLE observations 
MODIFY TTL 
    toDateTime(start_time) + INTERVAL 7 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID';

ALTER TABLE traces 
MODIFY TTL 
    toDateTime(timestamp) + INTERVAL 30 DAY 
    DELETE WHERE project_id = 'YOUR_PROJECT_ID';
```

## UI Verification

Verify the effects in the LangFuse web interface:

1. **Navigate to Project** - Go to your target project in LangFuse UI
2. **Check Traces Section** - Verify traces are limited to your retention period
3. **Monitor Performance** - Observe improved query response times
4. **Verify Compliance** - Confirm data retention aligns with your data protection policies

## Troubleshooting TTL Issues

### TTL Rules Not Taking Effect

**Problem**: Data older than retention period still exists

**Possible Causes**:
- TTL rules run during background merges, which may be infrequent
- Insufficient merge activity on low-traffic tables

**Solutions**:

```sql
-- Force merge to trigger TTL
OPTIMIZE TABLE traces FINAL;

-- Check TTL configuration
SHOW CREATE TABLE traces;

-- Verify TTL rules are applied
SELECT table, ttl_expression 
FROM system.tables 
WHERE database = 'default' AND table IN ('traces', 'observations', 'scores');
```

### Performance Impact During Cleanup

**Problem**: System performance degrades during TTL operations

**Solutions**:

- **Schedule Operations** - Run `OPTIMIZE TABLE FINAL` during maintenance windows
- **Gradual Cleanup** - Use shorter retention periods initially, then extend
- **Monitor Resources** - Watch CPU and I/O during cleanup operations

### Accidental Data Loss

**Problem**: TTL rules deleted data that should have been retained

**Prevention**:

- **Test First** - Apply TTL rules to development projects before production
- **Backup Strategy** - Ensure backups exist before implementing TTL
- **Gradual Rollout** - Start with short retention periods and extend as needed

**Recovery** (if backups exist):

```sql
-- Remove TTL rule
ALTER TABLE traces MODIFY TTL;

-- Restore from backup (implementation depends on backup strategy)
```

## Best Practices

### Planning and Implementation

1. **Start Conservative** - Begin with longer retention periods and reduce as needed
2. **Test Thoroughly** - Implement on development projects first
3. **Document Decisions** - Record retention periods and business justifications
4. **Monitor Impact** - Track storage usage and query performance improvements

### Operational Management

1. **Regular Reviews** - Periodically assess if retention periods meet business needs
2. **Automated Monitoring** - Set up alerts for TTL rule failures or unexpected data volumes
3. **Backup Coordination** - Ensure backup strategies account for TTL policies
4. **Compliance Alignment** - Verify TTL periods meet regulatory requirements

### Performance Optimization

1. **Batch Operations** - Group TTL rule changes to minimize system impact
2. **Off-Peak Scheduling** - Run manual cleanup operations during low-usage periods
3. **Resource Monitoring** - Watch for resource contention during cleanup operations
4. **Incremental Approach** - Implement TTL gradually across projects

## Integration with Other Processes

### Backup Strategy

Coordinate TTL rules with your backup strategy:

```bash
# Example: Backup before applying aggressive TTL
kubectl exec -it langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --query "
BACKUP TABLE traces TO S3('s3://backup-bucket/langfuse-backup/');"

# Then apply TTL rules
```

### Monitoring and Alerting

Set up monitoring for TTL effectiveness:

```bash
# Example monitoring query for alerting
kubectl exec -it langfuse-clickhouse-shard0-0 -n langfuse -- clickhouse-client --query "
SELECT 
    project_id,
    count() as records_exceeding_ttl
FROM traces 
WHERE timestamp < now() - INTERVAL 35 DAY  -- 5 days past 30-day TTL
GROUP BY project_id
HAVING records_exceeding_ttl > 0;"
```

## Next Steps

After implementing TTL rules:

**For Database Management**:

- **[Database Architecture](../explanation/database-architecture.qmd)** - Understand ClickHouse storage and performance characteristics
- **[ClickHouse Troubleshooting](clickhouse-troubleshooting.qmd)** - Resolve database-specific issues

**For Deployment Operations**:

- **[Deployment Troubleshooting](deployment-troubleshooting.qmd)** - General deployment and infrastructure issues
- **[User Management](user-management.qmd)** - Manage user accounts and permissions

**For Monitoring**:

- **[Observability Hierarchy](../explanation/observability-hierarchy.qmd)** - Understand how data flows through the system

---

*TTL rules provide automated data lifecycle management, ensuring your LangFuse deployment remains performant and cost-effective while meeting your data retention requirements.*
