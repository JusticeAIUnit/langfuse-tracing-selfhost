---
title: "LLM Application Tracing"
format:
  html:
    toc: true
diataxis:
  type: "how-to"
  purpose: "Problem-oriented - practical solutions for tracing LLM applications"
---

::: {.callout-note icon="false"}
**ðŸ› ï¸ How-to Guides** - This section provides practical solutions for common LLM tracing challenges. Each guide addresses a specific problem you might encounter.
:::

# LLM Application Tracing

This collection of how-to guides shows you how to solve specific problems when tracing Large Language Model applications with LangFuse.

## How to Trace OpenAI API Calls

**Problem**: You want to monitor OpenAI API usage, costs, and performance.

**Solution**: Use the LangFuse OpenAI integration for automatic tracing.

```python
from langfuse.openai import OpenAI
from langfuse.decorators import observe
import os

# Initialize OpenAI client with LangFuse
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    langfuse_public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    langfuse_secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    langfuse_host=os.getenv("LANGFUSE_HOST")
)

@observe()
def generate_response(prompt: str, model: str = "gpt-3.5-turbo") -> str:
    """Generate a response using OpenAI with automatic tracing."""
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=150
    )
    return response.choices[0].message.content

# Usage
response = generate_response("Explain machine learning in simple terms")
print(response)
```

**What this solves**: Automatic capture of token usage, costs, latency, and model parameters for every OpenAI API call.

## How to Add Context to Chat Conversations

**Problem**: You need to track multi-turn conversations with context and user information.

**Solution**: Use session tracking and conversation context.

```python
@observe()
def chat_with_context(messages: list, user_id: str = None) -> str:
    """Generate response with conversation context."""
    
    # Add system message for context
    system_message = {
        "role": "system", 
        "content": "You are a helpful AI assistant for the Ministry of Justice."
    }
    
    full_messages = [system_message] + messages
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=full_messages,
        temperature=0.7,
        max_tokens=500,
        user=user_id  # Track usage by user
    )
    
    return response.choices[0].message.content

# Usage
conversation = [
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI is artificial intelligence..."},
    {"role": "user", "content": "How is it used in government?"}
]

response = chat_with_context(conversation, user_id="user_123")
```

**What this solves**: Tracking conversation flow, user attribution, and context preservation across multiple turns.

## How to Implement RAG with Tracing

**Problem**: You need to monitor both the retrieval and generation phases of RAG systems.

**Solution**: Create separate traces for retrieval and generation with detailed metrics.

```python
from langfuse.decorators import observe
from langfuse import Langfuse
import numpy as np

langfuse = Langfuse()

@observe()
def embed_query(query: str) -> list:
    """Create embeddings for the query."""
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=query
    )
    return response.data[0].embedding

@observe()
def search_documents(query_embedding: list, top_k: int = 5) -> list:
    """Search for relevant documents using embeddings."""
    # Placeholder for document search logic
    # In practice, this would search your vector database
    return [
        {
            "id": "doc_1",
            "content": "AI in government improves efficiency...",
            "score": 0.95
        },
        {
            "id": "doc_2", 
            "content": "Machine learning helps automate processes...",
            "score": 0.88
        }
    ]

@observe()
def generate_rag_response(query: str, documents: list) -> str:
    """Generate response using retrieved documents."""
    
    # Construct prompt with retrieved context
    context = "\n".join([doc["content"] for doc in documents])
    
    prompt = f"""
    Context: {context}
    
    Question: {query}
    
    Please answer the question based on the provided context.
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=300
    )
    
    return response.choices[0].message.content

@observe()
def rag_pipeline(query: str) -> dict:
    """Complete RAG pipeline with tracing."""
    
    # Step 1: Embed query
    query_embedding = embed_query(query)
    
    # Step 2: Search documents
    relevant_docs = search_documents(query_embedding)
    
    # Step 3: Generate response
    response = generate_rag_response(query, relevant_docs)
    
    return {
        "query": query,
        "response": response,
        "sources": [doc["id"] for doc in relevant_docs],
        "source_count": len(relevant_docs)
    }

# Usage
result = rag_pipeline("How does AI help in government services?")
print(result)
```

**What this solves**: Visibility into retrieval quality, document relevance scores, and generation performance in RAG systems.

## How to Trace LangChain Applications

**Problem**: You need to monitor LangChain chains and their components.

**Solution**: Use the LangFuse callback handler for automatic LangChain tracing.

```python
from langfuse.callback import CallbackHandler
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize LangFuse callback handler
langfuse_handler = CallbackHandler(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST")
)

# Create LangChain components
llm = OpenAI(temperature=0.7)
prompt = PromptTemplate(
    input_variables=["question"],
    template="You are an AI assistant for the Ministry of Justice. Answer this question: {question}"
)
chain = LLMChain(llm=llm, prompt=prompt)

# Execute with tracing
result = chain.run(
    question="What are the benefits of AI in legal processes?",
    callbacks=[langfuse_handler]
)
print(result)
```

**What this solves**: Automatic tracing of LangChain chains, including prompt templates, LLM calls, and intermediate steps.

## How to Compare Multiple Models

**Problem**: You want to A/B test different models for the same task.

**Solution**: Create traces for each model and compare performance metrics.

```python
@observe()
def compare_models(prompt: str, models: list = None) -> dict:
    """Compare responses from different models."""
    
    if models is None:
        models = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
    
    results = {}
    
    for model in models:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=300
            )
            
            results[model] = {
                "response": response.choices[0].message.content,
                "tokens": response.usage.total_tokens,
                "cost": calculate_cost(response.usage)
            }
        except Exception as e:
            results[model] = {
                "error": str(e),
                "response": None
            }
    
    return results

def calculate_cost(usage) -> float:
    """Calculate estimated cost based on token usage."""
    # GPT-3.5-turbo pricing (example)
    input_cost = usage.prompt_tokens * 0.0015 / 1000
    output_cost = usage.completion_tokens * 0.002 / 1000
    return input_cost + output_cost

# Usage
comparison = compare_models("Explain the benefits of AI in healthcare")
```

**What this solves**: Side-by-side comparison of model performance, cost, and quality for the same input.

## How to Test Prompt Variants

**Problem**: You want to optimize prompts by testing different versions.

**Solution**: Create separate traces for each prompt variant to compare effectiveness.

```python
@observe()
def test_prompt_variants(query: str, user_id: str = None) -> dict:
    """Test different prompt variants."""
    
    prompts = {
        "formal": f"Please provide a formal response to: {query}",
        "casual": f"Hey! Can you help with this: {query}",
        "detailed": f"Please provide a comprehensive analysis of: {query}"
    }
    
    results = {}
    
    for variant, prompt in prompts.items():
        # Create a trace for each variant
        trace = langfuse.trace(
            name=f"prompt_variant_{variant}",
            input={"prompt": prompt, "variant": variant},
            user_id=user_id
        )
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=200
        )
        
        result = response.choices[0].message.content
        results[variant] = result
        
        trace.update(output={"response": result})
    
    return results

# Usage
variants = test_prompt_variants(
    "What is machine learning?",
    user_id="test_user"
)
```

**What this solves**: Systematic testing of different prompt approaches with clear performance tracking.

## How to Monitor Streaming Responses

**Problem**: You need to track streaming LLM responses for real-time applications.

**Solution**: Use manual tracing to capture streaming metrics.

```python
@observe()
def stream_response(prompt: str) -> str:
    """Handle streaming response with tracing."""
    
    full_response = ""
    start_time = time.time()
    
    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300,
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            full_response += content
            # Could add real-time monitoring here
    
    end_time = time.time()
    
    # Log streaming metrics
    langfuse.trace(
        name="streaming_response",
        input={"prompt": prompt},
        output={
            "response": full_response,
            "streaming_duration": end_time - start_time,
            "response_length": len(full_response)
        }
    )
    
    return full_response

# Usage
response = stream_response("Tell me about AI ethics")
```

**What this solves**: Monitoring streaming response performance and capturing complete conversation flows.

## How to Track Response Quality

**Problem**: You need to automatically assess and monitor response quality.

**Solution**: Implement quality scoring and log it as trace metadata.

```python
@observe()
def generate_with_quality_check(prompt: str) -> dict:
    """Generate response with quality assessment."""
    
    # Generate response
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300
    )
    
    content = response.choices[0].message.content
    
    # Assess quality
    quality_score = assess_response_quality(content)
    
    # Log quality score to LangFuse
    langfuse.score(
        name="response_quality",
        value=quality_score,
        comment=f"Quality assessment for prompt: {prompt[:50]}..."
    )
    
    return {
        "response": content,
        "quality_score": quality_score,
        "tokens_used": response.usage.total_tokens
    }

def assess_response_quality(content: str) -> float:
    """Simple quality assessment."""
    # Implement your quality metrics here
    # This is a placeholder example
    score = min(1.0, len(content) / 100)  # Simple length-based scoring
    
    # You might implement:
    # - Sentiment analysis
    # - Coherence scoring
    # - Factual accuracy checks
    # - Relevance scoring
    
    return score

# Usage
result = generate_with_quality_check("Explain quantum computing")
```

**What this solves**: Automated quality monitoring and performance tracking for LLM outputs.

## How to Handle Rate Limiting

**Problem**: You need to manage API rate limits and track failures.

**Solution**: Implement retry logic with exponential backoff and trace all attempts.

```python
import time
from langfuse.decorators import observe

@observe()
def robust_llm_call(prompt: str, max_retries: int = 3) -> dict:
    """Make LLM call with retry logic and rate limit handling."""
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=200
            )
            
            return {
                "response": response.choices[0].message.content,
                "attempt": attempt + 1,
                "success": True
            }
            
        except Exception as e:
            if "rate limit" in str(e).lower() and attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                time.sleep(wait_time)
                continue
            else:
                # Log the failure
                langfuse.trace(
                    name="llm_call_failed",
                    input={"prompt": prompt, "attempt": attempt + 1},
                    output={"error": str(e), "success": False},
                    level="ERROR"
                )
                raise

# Usage
try:
    result = robust_llm_call("What is the capital of France?")
    print(result)
except Exception as e:
    print(f"Failed after all retries: {e}")
```

**What this solves**: Reliable LLM API calls with proper error handling and retry tracking.

## Next Steps

### **Continue Learning**
- **[Basic Python Tracing](basic.qmd)** - Master the fundamentals first

### **Solve More Complex Problems**
- **[Complex Workflows](workflow.qmd)** - Trace multi-step AI processes

### **Deploy and Monitor**
- **[Azure Deployment](../deployment/azure.qmd)** - Set up your own LangFuse instance

---

*These guides provide solutions for the most common LLM tracing challenges. Adapt the code examples to your specific use cases and requirements.* 