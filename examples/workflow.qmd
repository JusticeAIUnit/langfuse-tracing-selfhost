---
title: "Complex Workflow Tracing"
format:
  html:
    toc: true
diataxis:
  type: "how-to"
  purpose: "Problem-oriented - practical solutions for tracing complex multi-step workflows"
---

::: {.callout-note icon="false"}
**ðŸ› ï¸ How-to Guides** - This section provides practical solutions for tracing complex, multi-step workflows. Each guide addresses a specific workflow challenge.
:::

# Complex Workflow Tracing

This collection of how-to guides shows you how to solve specific problems when tracing complex, multi-step AI workflows with LangFuse.

## How to Trace Document Processing Pipelines

**Problem**: You need to monitor a multi-step document processing workflow with detailed visibility into each stage.

**Solution**: Use nested spans to represent pipeline stages with comprehensive error handling.

```python
from langfuse.decorators import observe
from langfuse import Langfuse
import time
import os

langfuse = Langfuse()

class DocumentProcessor:
    """Document processing pipeline with comprehensive tracing."""
    
    def __init__(self):
        # Initialize your processing dependencies here
        pass
    
    @observe()
    def process_document(self, document_path: str, user_id: str = None) -> dict:
        """Process a document through the complete pipeline."""
        
        try:
            # Step 1: Extract text
            text = self._extract_text(document_path)
            
            # Step 2: Analyze structure
            structure = self._analyze_structure(text)
            
            # Step 3: Extract entities
            entities = self._extract_entities(text)
            
            # Step 4: Generate summary
            summary = self._generate_summary(text)
            
            # Step 5: Create final report
            report = self._create_report(summary, entities, structure)
            
            result = {
                "document_path": document_path,
                "text_length": len(text),
                "structure": structure,
                "entities": entities,
                "summary": summary,
                "report": report,
                "processed_at": time.time()
            }
            
            return result
            
        except Exception as e:
            # Trace will automatically capture the error
            print(f"Pipeline failed: {e}")
            raise
    
    @observe()
    def _extract_text(self, document_path: str) -> str:
        """Extract text from document."""
        # Simulate text extraction
        time.sleep(0.1)
        return f"Extracted text from {document_path}"
    
    @observe()
    def _analyze_structure(self, text: str) -> dict:
        """Analyze document structure."""
        time.sleep(0.2)
        return {
            "type": "report",
            "sections": ["introduction", "body", "conclusion"],
            "word_count": len(text.split())
        }
    
    @observe()
    def _extract_entities(self, text: str) -> list:
        """Extract named entities from text."""
        time.sleep(0.3)
        return [
            {"entity": "sample_entity", "type": "organization"},
            {"entity": "another_entity", "type": "person"}
        ]
    
    @observe()
    def _generate_summary(self, text: str) -> str:
        """Generate document summary."""
        time.sleep(0.4)
        return f"Summary of document with {len(text)} characters"
    
    @observe()
    def _create_report(self, summary: str, entities: list, structure: dict) -> dict:
        """Create final processing report."""
        time.sleep(0.1)
        return {
            "summary": summary,
            "entity_count": len(entities),
            "structure_type": structure["type"],
            "confidence_score": 0.85
        }

# Usage
processor = DocumentProcessor()
result = processor.process_document("legal_document.pdf", user_id="analyst_123")
```

**What this solves**: Complete visibility into document processing pipelines with automatic error capture and performance monitoring for each stage.

## How to Trace Multi-Turn Conversations

**Problem**: You need to track conversation flows with context preservation across multiple turns.

**Solution**: Use session-based tracing to group related conversation turns.

```python
class ConversationHandler:
    """Handle multi-turn conversations with context tracking."""
    
    def __init__(self):
        self.sessions = {}
    
    @observe()
    def start_conversation(self, user_id: str) -> str:
        """Start a new conversation session."""
        
        session_id = f"session_{user_id}_{int(time.time())}"
        
        self.sessions[session_id] = {
            "user_id": user_id,
            "messages": [],
            "context": {},
            "started_at": time.time()
        }
        
        return session_id
    
    @observe()
    def process_message(self, session_id: str, message: str) -> dict:
        """Process a message in the conversation."""
        
        if session_id not in self.sessions:
            raise ValueError("Session not found")
        
        session = self.sessions[session_id]
        
        # Add user message to history
        session["messages"].append({
            "role": "user",
            "content": message,
            "timestamp": time.time()
        })
        
        # Process the message through your pipeline
        intent = self._classify_intent(message)
        entities = self._extract_entities(message)
        self._update_context(session, intent, entities)
        response = self._generate_response(session, message, intent)
        
        # Add assistant response to history
        session["messages"].append({
            "role": "assistant",
            "content": response,
            "timestamp": time.time()
        })
        
        return {
            "response": response,
            "intent": intent,
            "entities": entities,
            "session_id": session_id
        }
    
    @observe()
    def _classify_intent(self, message: str) -> str:
        """Classify user intent."""
        # Simplified intent classification
        if "?" in message:
            return "question"
        elif "help" in message.lower():
            return "help_request"
        else:
            return "statement"
    
    @observe()
    def _extract_entities(self, message: str) -> dict:
        """Extract entities from message."""
        return {
            "keywords": message.lower().split(),
            "length": len(message),
            "has_question": "?" in message
        }
    
    @observe()
    def _update_context(self, session: dict, intent: str, entities: dict):
        """Update conversation context."""
        context = session["context"]
        context["last_intent"] = intent
        context["message_count"] = len(session["messages"])
        
        # Track topics
        if "topics" not in context:
            context["topics"] = []
        context["topics"].extend(entities.get("keywords", []))
        context["topics"] = list(set(context["topics"]))  # Remove duplicates
    
    @observe()
    def _generate_response(self, session: dict, message: str, intent: str) -> str:
        """Generate contextual response."""
        context = session["context"]
        
        # Simple response generation based on intent
        if intent == "question":
            return f"I understand you're asking about: {message}"
        elif intent == "help_request":
            return "I'm here to help! What do you need assistance with?"
        else:
            return f"I see you mentioned: {message}"

# Usage
handler = ConversationHandler()
session_id = handler.start_conversation("user_123")
response1 = handler.process_message(session_id, "Hello, I need help with a legal question")
response2 = handler.process_message(session_id, "What are my rights as a tenant?")
```

**What this solves**: Comprehensive tracking of conversation flows with context preservation and intent analysis.

## How to Trace Batch Processing Operations

**Problem**: You need to monitor batch processing with detailed per-item tracking and error handling.

**Solution**: Use hierarchical tracing with item-level spans within batch operations.

```python
class BatchProcessor:
    """Process multiple items in parallel with detailed tracing."""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
    
    @observe()
    def process_batch(self, items: list, user_id: str = None) -> dict:
        """Process multiple items with detailed tracking."""
        
        results = []
        errors = []
        
        for i, item in enumerate(items):
            try:
                # Create a span for each item
                with langfuse.trace(
                    name=f"process_item_{i}",
                    input={"item": item, "index": i},
                    user_id=user_id
                ) as item_trace:
                    
                    result = self._process_single_item(item)
                    item_trace.update(output=result)
                    results.append(result)
                    
            except Exception as e:
                error_info = {
                    "item_index": i,
                    "item": item,
                    "error": str(e),
                    "error_type": type(e).__name__
                }
                errors.append(error_info)
                
                # Log error trace
                langfuse.trace(
                    name=f"process_item_{i}_error",
                    input={"item": item, "index": i},
                    output=error_info,
                    level="ERROR"
                )
        
        # Create summary
        summary = {
            "total_items": len(items),
            "successful": len(results),
            "failed": len(errors),
            "success_rate": len(results) / len(items) if items else 0,
            "results": results,
            "errors": errors
        }
        
        return summary
    
    @observe()
    def _process_single_item(self, item: str) -> dict:
        """Process a single item."""
        # Simulate processing
        time.sleep(0.1)
        
        # Simulate occasional failures
        if "error" in item.lower():
            raise ValueError(f"Processing failed for item: {item}")
        
        return {
            "item": item,
            "processed": True,
            "word_count": len(item.split()),
            "timestamp": time.time()
        }

# Usage
processor = BatchProcessor()
items = ["document 1", "document 2", "error document", "document 4"]
results = processor.process_batch(items, user_id="batch_user")
```

**What this solves**: Detailed tracking of batch operations with individual item success/failure rates and error analysis.

## How to Trace Quality Assurance Workflows

**Problem**: You need to implement multi-stage quality assessment with detailed scoring and compliance tracking.

**Solution**: Create separate traces for each quality assessment stage with scoring metadata.

```python
class QualityAssurance:
    """Multi-stage quality assessment workflow."""
    
    @observe()
    def assess_document_quality(self, document_path: str, content: str) -> dict:
        """Comprehensive quality assessment."""
        
        # Stage 1: Technical quality
        tech_quality = self._assess_technical_quality(content)
        
        # Stage 2: Content quality
        content_quality = self._assess_content_quality(content)
        
        # Stage 3: Compliance check
        compliance = self._check_compliance(content)
        
        # Stage 4: Final scoring
        final_score = self._calculate_final_score(
            tech_quality, content_quality, compliance
        )
        
        # Log quality scores
        langfuse.score(
            name="document_quality",
            value=final_score,
            comment=f"Quality assessment for {document_path}"
        )
        
        return {
            "document_path": document_path,
            "technical_quality": tech_quality,
            "content_quality": content_quality,
            "compliance": compliance,
            "final_score": final_score,
            "passed": final_score >= 0.7
        }
    
    @observe()
    def _assess_technical_quality(self, content: str) -> dict:
        """Assess technical quality metrics."""
        
        readability = self._calculate_readability(content)
        structure = self._assess_structure(content)
        
        return {
            "readability": readability,
            "length": len(content),
            "structure": structure,
            "score": (readability + structure) / 2
        }
    
    @observe()
    def _assess_content_quality(self, content: str) -> dict:
        """Assess content quality."""
        
        # Simulate content quality assessment
        clarity = len(content) / 1000  # Simple proxy
        completeness = min(1.0, len(content.split()) / 100)
        
        return {
            "clarity": clarity,
            "completeness": completeness,
            "score": (clarity + completeness) / 2
        }
    
    @observe()
    def _check_compliance(self, content: str) -> dict:
        """Check regulatory compliance."""
        
        return {
            "gdpr_compliant": True,
            "accessibility_compliant": True,
            "language_appropriate": True,
            "score": 0.92
        }
    
    @observe()
    def _calculate_readability(self, content: str) -> float:
        """Calculate readability score."""
        # Simplified readability calculation
        words = len(content.split())
        sentences = content.count('.') + content.count('!') + content.count('?')
        if sentences == 0:
            return 0.5
        return min(1.0, words / sentences / 20)
    
    @observe()
    def _assess_structure(self, content: str) -> float:
        """Assess document structure."""
        # Simple structure assessment
        has_paragraphs = '\n\n' in content
        has_headings = any(line.isupper() for line in content.split('\n'))
        return 0.8 if has_paragraphs and has_headings else 0.5
    
    @observe()
    def _calculate_final_score(self, tech: dict, content: dict, compliance: dict) -> float:
        """Calculate weighted final score."""
        
        weights = {
            "technical": 0.3,
            "content": 0.5,
            "compliance": 0.2
        }
        
        final_score = (
            tech["score"] * weights["technical"] +
            content["score"] * weights["content"] +
            compliance["score"] * weights["compliance"]
        )
        
        return round(final_score, 2)

# Usage
qa = QualityAssurance()
result = qa.assess_document_quality(
    "policy_document.pdf", 
    "This is a sample policy document. It contains important information about our procedures."
)
```

**What this solves**: Multi-stage quality assessment with detailed scoring, compliance tracking, and quality metrics.

## How to Implement Workflow Monitoring with Alerts

**Problem**: You need real-time monitoring of workflow performance with automatic alerting for issues.

**Solution**: Implement monitoring traces with threshold-based alerting.

```python
class WorkflowMonitor:
    """Monitor workflow performance and trigger alerts."""
    
    def __init__(self):
        self.thresholds = {
            "error_rate": 0.05,
            "response_time": 30.0,
            "quality_score": 0.7
        }
    
    @observe()
    def monitor_workflow(self, workflow_name: str, metrics: dict):
        """Monitor workflow and trigger alerts if needed."""
        
        alerts = []
        
        # Check error rate
        if metrics.get("error_rate", 0) > self.thresholds["error_rate"]:
            alerts.append({
                "type": "error_rate_high",
                "value": metrics["error_rate"],
                "threshold": self.thresholds["error_rate"],
                "severity": "HIGH"
            })
        
        # Check response time
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "response_time_high",
                "value": metrics["avg_response_time"],
                "threshold": self.thresholds["response_time"],
                "severity": "MEDIUM"
            })
        
        # Check quality score
        if metrics.get("avg_quality_score", 1.0) < self.thresholds["quality_score"]:
            alerts.append({
                "type": "quality_score_low",
                "value": metrics["avg_quality_score"],
                "threshold": self.thresholds["quality_score"],
                "severity": "MEDIUM"
            })
        
        if alerts:
            self._trigger_alerts(workflow_name, alerts)
        
        # Log monitoring results
        langfuse.trace(
            name="workflow_monitoring",
            input={"workflow": workflow_name, "metrics": metrics},
            output={"alerts": alerts, "status": "alert" if alerts else "healthy"},
            level="WARNING" if alerts else "INFO"
        )
        
        return {
            "workflow": workflow_name,
            "metrics": metrics,
            "alerts": alerts,
            "status": "alert" if alerts else "healthy"
        }
    
    @observe()
    def _trigger_alerts(self, workflow_name: str, alerts: list):
        """Trigger alerts for workflow issues."""
        
        for alert in alerts:
            # Create alert trace
            langfuse.trace(
                name="workflow_alert",
                input={
                    "workflow": workflow_name,
                    "alert_type": alert["type"],
                    "value": alert["value"],
                    "threshold": alert["threshold"]
                },
                output={"alert": alert},
                level="WARNING" if alert["severity"] == "MEDIUM" else "ERROR",
                tags=["alert", "monitoring", workflow_name]
            )
            
            # In practice, you might send notifications here
            print(f"ALERT: {alert['type']} for {workflow_name}")

# Usage
monitor = WorkflowMonitor()
sample_metrics = {
    "error_rate": 0.08,  # Above threshold
    "avg_response_time": 25.0,  # Below threshold
    "avg_quality_score": 0.6  # Below threshold
}

result = monitor.monitor_workflow("document_processing", sample_metrics)
```

**What this solves**: Real-time workflow monitoring with automatic alerting, threshold management, and performance tracking.

## Next Steps

### **Continue Learning**
- **[Basic Python Tracing](basic.qmd)** - Master the fundamentals first

### **Solve Specific Problems**
- **[LLM Applications](llm.qmd)** - Apply tracing to AI applications

### **Deploy and Monitor**
- **[Azure Deployment](../deployment/azure.qmd)** - Set up your own LangFuse instance
- **[Environment Configuration](../deployment/config.qmd)** - Configure your deployment

---

*These guides provide solutions for the most common complex workflow tracing challenges. Adapt the patterns to your specific use cases and requirements.* 